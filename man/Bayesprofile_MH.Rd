% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/Bayesprofile_MH.R
\name{Bayesprofile_MH}
\alias{Bayesprofile_MH}
\title{Bayesian Profiling using the Metropolis-Hastings Algorithm}
\usage{
Bayesprofile_MH(dist, mu0, inf0, M, it0, flag_smooth = FALSE)
}
\arguments{
\item{dist}{a list containing the distribution information of both the aggregate data and target list (list)}

\item{mu0}{a vector specifying the mean of the prior distribution (vector)}

\item{inf0}{a matrix specifying the information matrix of prior distribution (matrix)}

\item{M}{an integer specifying MCMC iteration numbers (int)}

\item{it0}{an integer specifying burn-in iteration numbers (int)}

\item{flag_smooth}{an optional logical variable specifying Dirichlet smoothing or not (bool)}
}
\value{
\code{Bayesprofile_MH} returns a list containing the following fields:
\item{wmc}{the posterior markov chain}
\item{nwmc}{normalized markov chain}
\item{Smc}{probabilities of being included in the list in markov chain}
\item{loglikmc}{log-likelihood of markov chain}
\item{rejectmc}{the rejection record of the markov chain}
\item{dist_plot}{The plot of posterior distribution}
}
\description{
\code{Bayesprofile_MH} estimates the probabilities of being included in the target list
conditional on unobserved group S using the observed indicator X using the Metropolis-Hastings Algorithm.
}
\details{
One thing that users could self-define is to use the Hamiltonian Monte Carlo algorithm (HMC) over the
Metropolis-Hastings(MH) method in this function. Generally, the HMC performs better than the MH algorithm:
Given the same length of generated chains, the HMC chains usually reaches the steady-state sooner and have
a lower log-likelihood. It is hard to incorporate stan in this package, but we gave a block of code in the
example to show how to use the HMC algorithm to do Bayesian Profiling, so that the user could run it on their
end and do the comparison.
}
\examples{

# data preparation
data(VillagesSample)
data(XS_pop)
Election = data_pre(VillagesSample,XS_pop,category = "zipcode_group",base= 'target')
# prior mu = 0, cov = 0.01, M = 4000, burn-in iterations = 200
Result_MH = Bayesprofile_MH(Election, 0, 0.01, 4000, 2000, FALSE)
# posterior distribution plot
Result_MH$dist_plot

library('rstan')
library('reshape2')
library("inline")
library("bayesplot")
library("ggplot2")
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())



prmu0=rep(0,dists_Election$n_S)
prinf0=diag(dists_Election$n_S)*0.01

data_stan <- list(
  ncatX = dists_Election$n_X,
  ncatS = dists_Election$n_S,
  log_pXcondS = dists_Election$log_pXcondS,
  mu = prmu0,
  sigma = prinf0,
  y = dists_Election$target
)

election.stan = 'functions {
  vector getsimples(vector countX, vector wstab, int ncatS, int ncatX, matrix log_pXcondS){
    matrix[ncatX, ncatS] logprofcondX;
    vector[ncatX] nconst;
    vector[ncatS] S;

    for (i in 1:ncatX){
      for (j in 1:ncatS){
        logprofcondX[i,j] = log_pXcondS[i,j] + wstab[j];
      }
      nconst[i] = log_sum_exp(logprofcondX[i,]);
      for (j in 1:ncatS){
        logprofcondX[i,j] = logprofcondX[i,j] - nconst[i];
      }
    }

    for (i in 1:ncatS){
      S[i] = 0;
      for (j in 1:ncatX){
        S[i] = S[i] + countX[j] * exp(logprofcondX[j,i]);
      }
    }

    return S;
  }

  real bayesprofile_lpdf(real y, vector wstab, int ncatS, row_vector log_pXcondS){

     //first define additional local variables
     real lprob;
     vector[ncatS] loglik;
     real hilfmax;

     lprob = 0;
     hilfmax = 0;
     for (i in 1:ncatS){
       loglik[i] = 0;
     }

     for (i in 1:ncatS){
       loglik[i] = wstab[i] + log_pXcondS[i];
     }
     hilfmax = max(loglik);

     for (i in 1:ncatS){
       loglik[i] = loglik[i] - hilfmax;
     }
     lprob = (log_sum_exp(loglik) + hilfmax) * y;

     return lprob;
  }
}

data {
  // Data inputs
  int ncatX; // number of observations (observed groups)
  int ncatS; // number of covariates (unobserved groups) (s1,s2...)
  matrix[ncatX, ncatS] log_pXcondS; //logp(X|S)
  vector[ncatS] mu; // vector of prior mean
  matrix[ncatS, ncatS] sigma; // matrix of prior covariance matrix
  vector[ncatX] y; // vector of target list
}

parameters {
  // the parameters we want to estimate would go in here
  vector[ncatS] w; // vector of probabilities to be in unobserved groups
}

transformed parameters {
  vector[ncatS] wstab;
  for (i in 1:ncatS){
    wstab[i] = w[i] - max(w);
  }
  wstab = wstab - log_sum_exp(wstab);
}

model {
  // This is where the probability model we want to estimate would go
  // Define the priors
  w ~ multi_normal(mu, sigma);
  // The likelihood
  for (i in 1:ncatX){
    y[i] ~ bayesprofile(wstab, ncatS, log_pXcondS[i,]);
  }
}

generated quantities {
  vector[ncatS] S;
  vector[ncatS] nw;
  nw=exp(wstab);
  S = getsimples(y, wstab, ncatS, ncatX, log_pXcondS);
}
'
# run the stan code
fit <- stan(model_code=election.stan,
            data = data_stan,
            chains = 4,
            iter = 2000)


posterior <- as.array(fit)

color_scheme_set("red")
my_labels <- c('ABSTENTION','BLANK','SARKOZY','ROYAL','BAYROU','LE PEN','BESANCENOT',
               'VILLIERS','BUFFET','VOYNET','BOVE','LAGUILLER','NIHOUS','SCHIVARDI')
mcmc_intervals(posterior, pars = c("S[1]", "S[2]", "S[14]", "S[10]",
                                   "S[6]", "S[12]", "S[3]", "S[9]",
                                   "S[4]", "S[8]", "S[7]", "S[13]",
                                   "S[11]", "S[5]")) + scale_y_discrete(labels = my_labels)


nw <- extract(fit, 'nw[1]')
nw <- unlist(nw, use.names = FALSE)

plot(density(nw),
     xlab=expression(nw), col=grey(0, 0.8),
     main="Parameter distribution")
}
\references{
De Bruyn, Arnaud, and Thomas Otter (2022)
\cite{Bayesian Consumer Profiling: How to Estimate Consumer Characteristics from Aggregate Data},
Journal of Marketing Research (2022), 59(4), 755â€“774
}
